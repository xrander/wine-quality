---
title: "The Algorithmic Grape"
subtitle: "Decoding Wine Quality with Machine Learning"
title-block-banner: true
title-block-banner-color: "orangered"
author: "Olamide Adu"
date: May 03, 2024
date-format: "MMMM DD, YYYY"
format: 
    html:
      theme:
          light: sandstone
          dark: darkly
      include-in-header:
        - text: <link rel="shortcut icon" href="image/favicon.ico" />
execute: 
  cache: true
fig-width: 10
---

# Introduction

<img src="image/wine_chemical.webp" alt="wine qualities with chemicals" width="90%" height="70%"/> 

<br>

Wine appreciation is an art form enjoyed by many. However, beyond the subjective experience of taste, there lies a science behind wine quality. Winemakers strive to produce exceptional vintages by carefully controlling various factors during the production process. This project explores the fascinating world of wine quality, specifically focusing on the ability to predict it based on measurable chemical properties.

## Objective

This project aims to develop a model that can estimate the quality of a wine using its chemical composition. By analyzing features such as acidity, residual sugar content, and sulfur dioxide levels, we aim to unlock valuable insights into the factors that contribute to a superior wine. This model can potentially benefit wine producers, retailers, and even consumers seeking a more informed approach to wine selection.

# Data

## Data Source

The data for this project was obtained from the [**UCI Machine Learning Repository**](https://archive.ics.uci.edu), a renowned resource for publicly available datasets. This repository offers a wealth of datasets for various machine learning tasks, making it a valuable resource for researchers and data scientists.

The specific dataset we utilized is titled [**Wine Quality**](https://archive.ics.uci.edu/dataset/186/wine+quality) and focuses on Portuguese **Vinho Verde** wines. <br> <img src="image/vinho_logo.jpg" alt="Vinho wine logo" width="90%" height="50%"/> <br>

Vinho Verde is a light-bodied wine known for its crisp acidity and refreshing taste. The dataset encompasses two separate datasets, one for red wines and another for white wines. For this project, we chose to focus on the white wine dataset.

## Data Definition

The dataset contains more than 4500 wine samples, each characterized by 11 chemical properties measured during the winemaking process. These features include factors like:

-   **Fixed Acidity**: Level of acidity due to tartaric acid.

-   **Volatile Acidity**: Level of acidity due to volatile acids like acetic acid.

-   **Alcohol**: Percentage of alcohol content by volume

-   **Residual Sugar**: Amount of remaining sugar after fermentation.

-   **pH**: Measure of acidity on a logarithmic scale.

-   **Citric Acid**: Minute quantity naturally present in grapes. Winemakers may add small amounts to increase tartness and prevent haze formation.

-   **Chlorides**: Level of chloride salts that can influence a wine's saltiness and bitterness.

-   **Free Sulfur Dioxide**: Amount of unbound sulfur dioxide (SO2) gas, a preservative commonly used to prevent spoilage by bacteria and oxidation.

-   **Total Sulfur Dioxide**: The total level of SO2 gas, including both free and bound forms.

-   **Density**: The relative density of the wine compared to water. It can be an indicator of the wine's alcohol content and sugar level.

-   **Sulphates**: The level of sulfate salts, which can influence a wine's overall mouth-feel and perception of bitterness.

-   **Quality**: This is the target variable, a score between 0 and 10 representing the perceived sensory quality of the wine according to human experts. This score serves as the target variable for our machine learning model, allowing us to predict the quality of a new wine based on its chemical makeup.

## Setting up the Analysis Environment

<img src = "image/cat-wine.webp" alt="wine qualities with chemicals" width="90%" height="70%"/>

<br>

To begin our analysis, we'll import essential libraries using the `pacman` package for efficient dependency management.

```{r}
#| label: import-libraries
#| message: false
#| warning: false

library(pacman)

p_load(tidyverse, tidymodels, gt, kableExtra, patchwork)
```

# Exploring the Data

Next, we'll import the wine quality dataset and get a glimpse of its contents. Here, we directly use the `janitor` package because it's only needed for this specific step. This approach avoids unnecessary library loading throughout the document.

```{r}
#| label: import-data
#| message: false
#| warning: false
wine_tbl <- read_delim("data/winequality-white.csv", delim = ";") |> 
  janitor::clean_names()
```

## Data Inspection: A Crucial Step

A crucial step in data analysis, after defining our goals and collecting data, is to inspect its quality and structure. This involves examining the data to identify potential issues like missing values, inconsistencies, or formatting errors. @tbl-data-preview below gives a preview of our data.

```{r}
#| label: tbl-data-preview
#| tbl-cap: "Data Preview"
#| echo: false

head(wine_tbl) |> 
  gt() |> 
  cols_label(
    fixed_acidity = "Fixed",
    volatile_acidity = "Volatile",
    citric_acid = "Citric Acid",
    residual_sugar = "Residual Sugar",
    chlorides = "Chlorides",
    free_sulfur_dioxide = "Free",
    total_sulfur_dioxide = "Total",
    density = "Density",
    p_h = "pH",
    sulphates = "Sulphates",
    alcohol = "Alcohol",
    quality = "Quality"
  ) |> 
  tab_spanner(
    label = "Acidity",
    columns = contains("acidity")
  ) |> 
  tab_spanner(
    label = "Sulfur dioxide",
    columns = contains("dioxide")
  ) |> 
  tab_header(
    title = "Data Preview: Wine Quality for White Wine"
  ) |> 
  opt_stylize(
    style = 3,
    color = "gray"
  ) |> 
  as_raw_html()
```

```{r}
#| label: data-summary
glimpse(wine_tbl)
```
Using the `glimpse` function we can get a high-level overview of the wine quality data and the data follows the expected structure in the data definition. We have `r dim(wine_tbl)[1]` rows and `r dim(wine_tbl)[2]` columns. All the columns are numeric variables.

```{r}
#| label: tbl-summary
#| tab-cap: Wine data summary
skimr::skim(wine_tbl) |> 
  select(-skim_type) |> 
  gt() |> 
  cols_label(
    skim_variable = "Variable",
    n_missing = "Missing Value",
    complete_rate = "Complete",
    numeric.mean = "Mean",
    numeric.sd = "Standard Deviation",
    numeric.p0 = "Min",
    numeric.p25 = "1st Percentile",
    numeric.p50 = "Median",
    numeric.p75 = "3rd Percentile",
    numeric.p100 = "Max",
    numeric.hist = "Histogram"
  ) |> 
  tab_header(
    title = "White Wine Quality Descriptive Statistics"
  ) |> 
  opt_stylize(
    style = 3,
    color = "gray"
  ) |> 
  as_raw_html()
```

@tbl-dat-sum summarizes the key characteristics of our data. It reveals:

-   **No Missing Values**: There are no missing data points, ensuring a complete dataset for our analysis.

-   **Descriptive Statistics**: @tbl-sum-dat presents various descriptive statistics for each variable. This includes the minimum and maximum values highlighting the data's range. Additionally, the first and third quartile provides an insight of the distribution of values in the data. Finally the mean and standard deviation indicate the average and spread of each variables relative to their mean.

-   **Outliers**: Initial exploration of the data suggests the presence of potential outliers in several features. Specifically, the maximum values for variables `fixed_acidity`, `volatile_acidity`, `citric_acid`, `residual_sugar`, `chlorides`, `free_sulphur_dioxide`, and `total_sulfur_dioxide` seems deviate significantly from the rest of the distribution.

The next step is to statistically confirm these observations, we can't rely solely on inspecting the maximum values. Instead, we'll employ appropriate outlier detection tests. Due to the fairly large data size we have, we can use either of:

-   **Grubb's** test

-   **Rosner's test**

These outlier detection tests help us statistically determine if the observed extreme values are likely outliers within the dataset. Both tests listed here are particularly useful for identifying outliers in a dataset that is approximately normally distributed.

## Univariate Inspection

### Dealing with Outliers

Before diving into formal tests, it's important we get a sense of our data. We can visualize each variables of our data to inspect their distribution. \#### Visual Inspection

```{r}
#| label: fig-normality-plot
#| fig-cap: |
#|    (a) shows the qqplot with variables like total and free sulful_dioxide and p_h
#|    approximately following normal distribution,(b) exposes the distribution of all
#|    variables which are mostly right-skewed.
#| fig-subcap: 
#|    - "QQ plot of all the variables shows they are not normally distributed"
#|    - "histogram of all variables showing tails"
#| layout-nrow: 2
#| message: false
#| warning: false

wine_tbl_longer <- wine_tbl |> 
  pivot_longer(
    cols = everything(),
    names_to = "variable",
    values_to = "value"
  )

wine_tbl_longer |> 
  filter(variable != "quality") |> 
  ggplot(aes(sample = value)) +
  geom_qq(alpha = .2, col = "violetred4") +
  geom_qq_line(col = "gray3") +
  facet_wrap(~variable, scales = "free") +
  labs(
    x = "Theoretical distribution",
    y = "Sample",
    title = "Variables are not normally distributed"
  ) +
  theme_minimal()

wine_tbl_longer |> 
  filter(variable !="quality") |> 
  ggplot(aes(value)) +
  facet_wrap(~variable, scales = "free") +
  geom_histogram(fill = "burlywood", col = "black") +
  geom_density(
    stat = "bin",
    col = "violetred"
  ) +
  labs(
    x = "Value",
    y = "Frequency",
    title = "Variables are fairly normal"
  ) +
  theme_minimal()
```

@fig-normality-plot shows some fairly normal data, but it won't be bad to test for normality using Shapiro-wilk's test. We can also use the `qqPlot()` function from the `car` package which I really like to test for the normality.

#### Shapiro Test

```{r}
#| label: tbl-normality-test
#| tbl-cap: "Shapiro-wilk's normality test: All varibles are not normally distributed"

normality_test <- wine_tbl |> 
  map(~tidy(shapiro.test(.))) |> 
  bind_rows(.id = "variable") |> 
  select(-method) |> 
  janitor::clean_names() |> 
  mutate(
    variable = str_to_title(str_replace_all(variable, "_", " "))
  )
actual_colnames <- colnames(normality_test)
desired_colnames <- str_to_title(names(normality_test))
names(desired_colnames) <- actual_colnames
  
normality_test |> 
  filter(variable != "Quality") |> 
  gt()  |> 
  cols_label(.list = desired_colnames) |> 
  tab_header(
    title = "Shapiro-wilk Normality Test"
  ) |> 
  opt_stylize(
    style = 3,
    color = "gray"
  ) |> 
  as_raw_html()
```

#### Cars qqPlot Test

```{r}
#| label: fig-qqplot-var
#| warning: false
#| message: false
#| fig-cap: Normality test with the car's package qqPlot function
#| fig-subcap: 
#|  - "pH"
#|  - "alcohol"
#| layout-nrow: 2

car::qqPlot(
  wine_tbl$p_h,
  ylab = paste0("pH"),
  main = "Quantile-quantile plot for pH variable"
)

car::qqPlot(
  wine_tbl$alcohol,
  ylab = paste0("pH"),
  main = "Quantile-quantile plot for alcohol variable"
)

```

@tbl-normality-test shows all the variables do not follow a normal distribution, @fig-qqplot-var stresses this and we can clearly see the presence of outlier.

### Where Domain Knowledge Shines

While our initial exploration identified potential outliers in the data, it's important to consider them in the context of wine making expertise. Domain knowledge from experienced professionals like [Cathy Howard](https://whicherridge.com.au/author/whicherridge/) suggests that the expected range for residual sugar in wine can vary widely, from as low as 0.3 g/L to over 35 g/L (as referenced in this [post](https://whicherridge.com.au/blog/what-is-residual-sugar-in-wine/)).

This information helps us understand that some seemingly extreme values in our data might actually be plausible for certain wine types. Similarly, consulting resources like Waterhouse's post on ["What's in Wine"](https://waterhouse.ucdavis.edu/whats-in-wine). can provide valuable insights into the expected ranges for other wine components.

Given the new information about our data and the supposed outliers we investigated we can proceed with our analysis ***thanks to domain knowledge***.

## Response Variable (Quality)

There are two ways we can proceed with our analysis based on our response variable, we can either take quality as a categorical variable or a continuous variable, I prefer and will proceed with the later.

```{r}
#| label: fig-qual-bar
#| fig-cap: |
#|  Frequency of the white wine quality ratings shows that the wine is mostly rate
#|    between 5 to 7
wine_tbl |> 
  ggplot(aes(quality)) +
  geom_histogram(
    binwidth = 1,
    fill = "violetred4"
  ) +
  geom_density(
    stat = "count",
    col = "orangered"
  ) +
  scale_x_continuous(
    breaks = seq(1, 10, 1)
  ) +
  labs(
    x = "Quality",
    y = "Frequency",
    title = "White wine quality frequency"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = .5, face = "bold")
  )
```

The distribution of wine ratings in @fig-qual-bar shows a range between 3 and 9. This is the same as what we have in @tbl-summary. White wines rated 6 and 5 are the most occurring wine.

## Correlation Matrix and Relationship Between Variables

It is important to see how the different variables relates to wine quality.

```{r}
#| label: fig-corrplot
#| fig-cap: Correlation Plot
#| warning: false
#| message: false

GGally::ggcorr(
  wine_tbl,
  method = "pairwise",
  geom = "circle",
  palette = "Spectral",
  max_size = 10,
  min_size = 2,
  label = TRUE,
  size = 3,
  hjust = .95
) +
  ggtitle("Wine Quality Feature Correlations") +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold")
  )
```

@fig-corrplot shows some noteworthy relationships between the variables. Here are the key observations:

-   **Quality and Alcohol**: Alcohol shows a moderate positive correlation with quality of wine
-   **Alcohol and Density**: A strong negative correlation exists between alcohol content and density. This implies that as alcohol level increases, the density of the wine decreases.
-   **Quality and Density:** There's moderate negative relationship between the quality and wine density.
-   **Residual Sugar and Density:** The density of wine is largely affected by the amount of residual sugar present in the wine
-   **Total Sulfur Dioxide and Free Sulfur Dioxide**: A moderate positive correlation is observed between total sulfur dioxide and free sulfur dioxide. This indicates that wines with higher total sulfur dioxide content also tend to have higher levels of unbound free sulfur dioxide.

Overall there's a fair distribution of correlation across all the variables ranging from as low as ± 0.1 to ± 0.8.

# Modeling: Predicting the Perfect Potion
<img src = "image/cat-taster.webp" alt="cat connoisseur" width="90%" height="70%"/> <br>

Imagine you're a wine connoisseur, swirling a glass and trying to guess how good a wine will be. That's kind of what we're setting out to do here, but with the help of some brainy computer programs!

We're going to test out three different "recipes" to predict white wine quality.  Think of them like different ways of mixing ingredients (the info we have about the wine) to come up with a final answer (how good the wine is). Here's the lowdown on each recipe:

-   Ordinary Least Squares (OLS) Regression: This is a foundational linear regression model that establishes a linear relationship between the independent variables and the quality score.

-   Decision Tree Model: This non-linear model uses a tree-like structure to make predictions by splitting the data based on a series of decision rules.

-   Random Forest: This ensemble method combines multiple decision trees, improving accuracy and reducing overfitting compared to a single tree.


## Data Splitting Strategy
To ensure robust model evaluation, we'll employ a data splitting strategy. This involves dividing the dataset into two portions:

-   Training Set (80%): This larger portion of the data will be used to train the models. The training process involves fitting the model parameters to the data, allowing the model to learn the underlying relationships between the features (independent variables) and the target variable (wine quality).

-   Test Set (20%): This smaller portion of the data will be reserved for testing the final model performance. We will not use the test set for training the models in any way. Instead, the trained model will be applied to the test set, and its predictions will be compared to the actual quality scores to assess the model's generalizability and accuracy on unseen data.

**Rationale for Splitting:**

The 80/20 split is a common choice for data splitting, but depending on the size of your dataset, a 70/30 split might also be acceptable.  The key idea is to have a sufficient amount of data for both training and testing purposes. The training set allows the model to learn, while the test set provides an unbiased assessment of how well the model performs on new data. To ensure balance in the distribution of the response variable (quality) between the test and the training data, we'll execute the split taking into account the distribution of the response variable, as seen in @fig-qual-bar the response variable is unevenly distributed.

To ensure reproducibility of the code, we'll also set.seed into 34433
```{r}
#| label: data-splitting

set.seed(34433)
wine_split <- initial_split(wine_tbl, prop = .8, strata = quality)
wine_train <- training(wine_split)
wine_test <- testing(wine_split)
```

We also need to evaluate the developed models, for this, we will use a cross-fold validation.
```{r}
#| label: create-crossfolds
set.seed(33323)
wine_folds <- vfold_cv(wine_train, v = 10)
```


## Model Specification

To ensure a good modeling workflow, we specify our model engines, and prepare to tune some parameters, for example, the decision tree model specification, we do not know the tree depth that would be optimal, so we tune this parameter. Other parameters which cannot be predetermine when fitting our model are also tuned.
```{r}
#| label: model-specs
#| hide: true

ols_spec <- linear_reg() |> # OLS specification
  set_engine("lm") |> 
  set_mode("regression")

dec_tree_spec <- decision_tree( # Decision tree specification
  tree_depth = tune(),
  cost_complexity = tune()
) |> 
  set_engine("rpart", importance = TRUE) |> 
  set_mode("regression")

rand_for_spec <- rand_forest( # Random forest specification
  trees = 1000,
  mtry = tune(),
  min_n = tune()
) |> 
  set_engine("ranger", importance = TRUE) |> 
  set_mode("regression")

xgboost_spec <- boost_tree(
  mtry = tune(),
  min_n = tune(),
  trees = tune(),
  learn_rate = .1
) |> 
  set_mode("regression") |> 
  set_engine("xgboost", importance = TRUE)

ols_spec
dec_tree_spec
rand_for_spec
xgboost_spec

```


## Feature Engineering
After model specification, we do some feature engineering based on the model specification, a good reference for recommended pre-processing or feature engineering techniques for different models can be found [here](https://www.tmwr.org/pre-proc-table). For our analysis, we will have models fitted on our variables without being preprocessed and models fitted with preprocessing. 

### Formula Recipe
Here, we assume no preprocessing, even if we ideally need to preprocess for our OLS model

```{r}
#| label: no-preproc

formula_rec <- recipe(
  quality ~ .,
  data = wine_train
)
formula_rec
```

### OLS preproc
For the second preprocessing, we do the following: 
- Perform a log transformation on the all predictors, `step_log`.

- Remove variables that is having no difference in their values - zero variance, `step_zv`.

This preprocessing is particularly beneficial to the OLS model

```{r}
#| label: ols-model-preproc
ols_rec <- recipe(
  quality ~ .,
  data =  wine_train
) |> 
  step_log(all_predictors()) |> 
  step_zv(all_predictors())

ols_rec
```

A preview of the preprocess object is given below @tbl-juice-preview
```{r}
#| label: tbl-juice-preview
#| tbl-cap: Variables after log transformation and removing zero variance variables
#| echo: false

prep(ols_rec) |> 
  juice() |> 
  head() |> 
  gt() |> 
  cols_label(
    fixed_acidity = "Fixed",
    volatile_acidity = "Volatile",
    citric_acid = "Citric Acid",
    residual_sugar = "Residual Sugar",
    chlorides = "Chlorides",
    free_sulfur_dioxide = "Free",
    total_sulfur_dioxide = "Total",
    density = "Density",
    p_h = "pH",
    sulphates = "Sulphates",
    alcohol = "Alcohol",
    quality = "Quality"
  ) |> 
  tab_spanner(
    label = "Acidity",
    columns = contains("acidity")
  ) |> 
  tab_spanner(
    label = "Sulfur dioxide",
    columns = contains("dioxide")
  ) |> 
  tab_header(
    title = "Preview of Preprocessed data"
  ) |> 
  opt_stylize(
    style = 3,
    color = "pink"
  ) |> 
  as_raw_html()
```


## Workflow
To account for all our modeling process, we create our workflow object, to capture, the recipe objects (*for preprocessing*) and our model specifications. We use the `workflow_set()` function for this, so we can use both recipe objects on each model


```{r}
#| label: design-workflow
wf_object <- workflow_set(
  preproc = list(
    no_preproc = formula_rec,
    log_zv = ols_rec
  ),
  model = list(
    ols = ols_spec,
    dec_tree = dec_tree_spec,
    rf = rand_for_spec,
    xg = xgboost_spec
  ),
  cross = TRUE
)
```

Using `workflow_map()`, we tune the models using a grid search on the resample object which we made earlier. We set `verbose = TRUE` to see the process

```{r}
wf_tune_fit <- workflow_map(
  wf_object,
  fn = "tune_grid",
  grid = 10,
  resamples = wine_folds,
  seed = 11,
  verbose = TRUE
)
```




## Parameter Tuning
Before tuning the models that requires tuning, viz, decision tree, and random forest, we set a search grid for them.

### Grid search
```{r}
#| label: grid-search

rf_grid <- grid_regular(
  min_n(),
  finalize(mtry(), wine_train),
  levels = 10
)

dec_tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 10
)
```

Now we can tune after setting the grid search
```{r}
#| label: tune

doParallel::registerDoParallel()
set.seed(234)

grid_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)

df_tune <- tune_grid(
  dec_tree_wf,
  resamples = wine_folds,
  control = grid_control,
  grid = dec_tree_grid
)

rf_tune <- tune_grid(
  rf_wf,
  grid = rf_grid,
  resamples = wine_folds,
  control = grid_control
)
```
[homepage](https://olamideadu.com)
